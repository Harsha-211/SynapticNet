{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7bf777-5280-4d34-97fa-0a8e99944b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())\n",
    "else:\n",
    "    print(\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c64397-3d6d-49ed-9584-69f90724f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, task_ids):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features \n",
    "        self.out_features = out_features\n",
    "        self.task_ids = task_ids\n",
    "        self.weight = nn.Parameter(torch.randn(out_features,in_features)*0.01)\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        self.register_buffer(\"neuron_ids\", torch.tensor(task_ids).long())\n",
    "\n",
    "    def forward(self, x, task_id):\n",
    "        out = F.linear(x,self.weight , self.bias)\n",
    "        mask = (self.neuron_ids == task_id).float().unsqueeze(0)\n",
    "        self._mask = self.neuron_ids == task_id\n",
    "\n",
    "        return out*mask\n",
    "\n",
    "    def apply_gradient_mask(self):\n",
    "        if self.weight.grad is not None:\n",
    "            inactive = ~self._mask\n",
    "            self.weight.grad[inactive] = 0\n",
    "            self.bias.grad[inactive]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559b8e06-3847-4906-90a2-6ef361f4484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynapticNet(nn.Module):\n",
    "    def __init__(self, input_size = 28*28 , hidden_size = 512 , output_szie =10 , hidden_layers = 10):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.hidden_layers):\n",
    "            task_ids = [1]*512\n",
    "            self.layers.append(TaskLinear(hidden_size, hidden_size,task_ids))\n",
    "        self.input_layer = TaskLinear(input_size,hidden_size,[1]*512)\n",
    "        self.output_layer = TaskLinear(hidden_size, output_szie , [1]*10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def grow(self, grow_size = 512 , output_grow_size = 10, task_id = [2]):\n",
    "        new_hidden_size = self.hidden_size+grow_size\n",
    "        new_output_size = self.output_layer.out_features + output_grow_size\n",
    "        new_task_ids = task_id*grow_size\n",
    "        old_task_ids = self.input_layer.task_ids\n",
    "        combined_task_ids = old_task_ids + new_task_ids\n",
    "        old_input_w = self.input_layer.weight.data.clone()\n",
    "        old_input_b = self.input_layer.bias.data.clone()\n",
    "        new_input = TaskLinear(self.input_layer.in_features , new_hidden_size , combined_task_ids)\n",
    "        with torch.no_grad():\n",
    "            new_input.weight[:self.hidden_size, :].copy_(old_input_w)\n",
    "            new_input.bias[:self.hidden_size].copy_(old_input_b)\n",
    "        self.input_layer = new_input\n",
    "\n",
    "        new_layers = nn.ModuleList()\n",
    "        for i in range(self.hidden_layers):\n",
    "            old_layer = self.layers[i]\n",
    "            old_w = old_layer.weight.data.clone()\n",
    "            old_b =  old_layer.bias.data.clone()\n",
    "            new_layer = TaskLinear(new_hidden_size , new_hidden_size , combined_task_ids)\n",
    "            with torch.no_grad():\n",
    "                new_layer.weight[:self.hidden_size, :self.hidden_size] = old_w\n",
    "                new_layer.bias[:self.hidden_size] = old_b\n",
    "            new_layers.append(new_layer)\n",
    "        self.layers = new_layers\n",
    "\n",
    "        old_out_w = self.output_layer.weight.data.clone()\n",
    "        old_out_b = self.output_layer.bias.data.clone()\n",
    "        old_out_task_ids = self.output_layer.task_ids\n",
    "        new_out_task_ids = task_id*output_grow_size\n",
    "        combined_out_task_ids = old_out_task_ids + new_out_task_ids\n",
    "        new_output = TaskLinear(new_hidden_size,new_output_size,combined_out_task_ids)\n",
    "        with torch.no_grad():\n",
    "            new_output.weight[:self.output_layer.out_features , :self.hidden_size].copy_(old_out_w)\n",
    "            new_output.bias[:self.output_layer.out_features].copy_(old_out_b)\n",
    "        self.output_layer = new_output\n",
    "\n",
    "        self.hidden_size = new_hidden_size\n",
    "    \n",
    "    def forward(self,x,task_id):\n",
    "        x = self.relu(self.input_layer(x,task_id))\n",
    "        for layer in self.layers:\n",
    "            x = self.relu(layer(x,task_id))\n",
    "        x = self.output_layer(x,task_id)\n",
    "        return x\n",
    "        \n",
    "    def apply_task_gradient_mask(self):\n",
    "        self.input_layer.apply_gradient_mask()\n",
    "        for layer in self.layers:\n",
    "            layer.apply_gradient_mask()\n",
    "        self.output_layer.apply_gradient_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe3c1314-08f6-4bc8-8bc1-b545fa416c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, dataset, task_id, label_offset=0):\n",
    "        self.dataset = dataset\n",
    "        self.task_id = task_id\n",
    "        self.label_offset = label_offset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x.view(-1), y + self.label_offset, self.task_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c597825e-92fd-4b59-9a6c-a60f7d03d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    fmnist = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    mnist = TaskDataset(mnist, task_id=1, label_offset=0)\n",
    "    fmnist = TaskDataset(fmnist, task_id=2, label_offset=10)\n",
    "\n",
    "    return DataLoader(mnist, batch_size=batch_size, shuffle=True), DataLoader(fmnist, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66270d46-de67-4af4-a482-b252b6294a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task(model, task_id, dataloader, epochs=3, lr=1e-3):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x,y, tid in dataloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            out = model(x, task_id)\n",
    "            loss = criterion(out,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model.apply_task_gradient_mask()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        acc = 100 * correct / total\n",
    "        print(f\"[Task {task_id}] Epoch {epoch+1} | Loss: {total_loss:.4f} | Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1e7212-900f-4652-ac54-e8f4b07609c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SynapticNet().to(device)\n",
    "mnist_loader, fmnist_loader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c398b372-cbca-4675-ad42-22de06b5ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on MNIST (Task 1)\n",
      "[Task 1] Epoch 1 | Loss: 368.7942 | Accuracy: 89.34%\n",
      "[Task 1] Epoch 2 | Loss: 329.8951 | Accuracy: 90.33%\n",
      "[Task 1] Epoch 3 | Loss: 283.3505 | Accuracy: 91.89%\n",
      "[Task 1] Epoch 4 | Loss: 244.8758 | Accuracy: 92.88%\n",
      "[Task 1] Epoch 5 | Loss: 222.1942 | Accuracy: 93.61%\n",
      "[Task 1] Epoch 6 | Loss: 208.6383 | Accuracy: 93.93%\n",
      "[Task 1] Epoch 7 | Loss: 196.4845 | Accuracy: 94.24%\n",
      "[Task 1] Epoch 8 | Loss: 184.1229 | Accuracy: 94.59%\n",
      "[Task 1] Epoch 9 | Loss: 192.5516 | Accuracy: 94.25%\n",
      "[Task 1] Epoch 10 | Loss: 170.7890 | Accuracy: 95.05%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Training on MNIST (Task 1)\")\n",
    "train_task(model, task_id=1, dataloader=mnist_loader , epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2def60-6099-4f7d-b422-46e0ec61d47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SynapticNet(\n",
       "  (layers): ModuleList(\n",
       "    (0-9): 10 x TaskLinear()\n",
       "  )\n",
       "  (input_layer): TaskLinear()\n",
       "  (output_layer): TaskLinear()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grow(grow_size=512 , output_grow_size=10, task_id=[2])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb397d8-b7f4-492c-a615-7ee306144cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on FMNIST (Task 2)\n",
      "[Task 2] Epoch 1 | Loss: 997.7255 | Accuracy: 57.73%\n",
      "[Task 2] Epoch 2 | Loss: 666.7542 | Accuracy: 74.37%\n",
      "[Task 2] Epoch 3 | Loss: 584.0280 | Accuracy: 78.60%\n",
      "[Task 2] Epoch 4 | Loss: 514.6503 | Accuracy: 81.29%\n",
      "[Task 2] Epoch 5 | Loss: 467.1802 | Accuracy: 83.34%\n",
      "[Task 2] Epoch 6 | Loss: 435.0229 | Accuracy: 84.80%\n",
      "[Task 2] Epoch 7 | Loss: 404.4991 | Accuracy: 85.64%\n",
      "[Task 2] Epoch 8 | Loss: 407.8861 | Accuracy: 85.45%\n",
      "[Task 2] Epoch 9 | Loss: 385.7003 | Accuracy: 86.48%\n",
      "[Task 2] Epoch 10 | Loss: 384.5262 | Accuracy: 86.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Training on FMNIST (Task 2)\")\n",
    "train_task(model, task_id=2, dataloader=fmnist_loader , epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adca5192-cd93-41c4-a7c7-a1114c579249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(model, task_id, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, tid in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x, task_id)\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"[Task {task_id}] Evaluation Accuracy: {acc:.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89167b22-6654-44f0-94a9-0aa787a1b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " testing on MNIST(Task 1)\n",
      "\n",
      "[Task 1] Evaluation Accuracy: 89.59%\n",
      "\n",
      " testing on FMNIST(Task 1)\n",
      "\n",
      "[Task 2] Evaluation Accuracy: 86.85%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.84666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n testing on MNIST(Task 1)\\n\")\n",
    "evaluate_task(model , task_id = 1, dataloader=mnist_loader)\n",
    "print(\"\\n testing on FMNIST(Task 1)\\n\")\n",
    "evaluate_task(model , task_id = 2, dataloader=fmnist_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b11aab-096d-4e0e-b191-d2d1e2cd22d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
